{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KT-ASSISTANT ###\n",
    "### NLP Project Group 8 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class BQDataset():\n",
    "    def __init__(self, path):\n",
    "        self.dataset = open(path,encoding=\"utf-8\")\n",
    "\n",
    "        self.dataset = [json.loads(instance) for instance in self.dataset ]\n",
    "\n",
    "        self.passages = []\n",
    "        self.questions = []\n",
    "        self.answers = []\n",
    "        self.titles = []\n",
    "\n",
    "        for inst in self.dataset:\n",
    "            self.passages.append(inst[\"passage\"])\n",
    "            self.questions.append(inst[\"question\"])\n",
    "            self.answers.append(inst[\"answer\"])\n",
    "            self.titles.append(inst[\"title\"])\n",
    "\n",
    "    def get_dataset(self):\n",
    "        return self.dataset\n",
    "\n",
    "    def get_split(self):\n",
    "        return self.passages, self.questions, self.answers\n",
    "\n",
    "bqd = BQDataset(\"datasets/train.jsonl\")\n",
    "dataset = bqd.get_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean(text, stem_words=True):\n",
    "    import re    # for regular expressions\n",
    "    from string import punctuation\n",
    "    from nltk.stem import SnowballStemmer    #if you are brave enough to do stemming\n",
    "    from nltk.corpus import stopwords      #if you want to remove stopwords\n",
    "    \n",
    "    if type(text) != str or text=='':\n",
    "        return ''\n",
    "\n",
    "    text = re.sub(\"\\'s\", \" \", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
    "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    text = re.sub(\"can't\", \"cannot\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"don't\", \"do not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"won't\", \"will not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"shouldn't\", \"should not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"couldn't\", \"could not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"isn't\", \"is not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"wasn't\", \"was not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"weren't\", \"were not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"haven't\", \"have not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"hasn't\", \"has not\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(r\"[0-9]-[0-9]\", \" minus \", text)\n",
    "    text = re.sub(\"-\", \" \", text)\n",
    "\n",
    "    digit_letters = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "    for i in range(len(digit_letters)):\n",
    "        regex = rf\"(?<=\\b){str(i)}(?=\\b)\"\n",
    "        text = re.sub(regex, digit_letters[i], text)\n",
    "    \n",
    "    # remove comma between numbers, i.e. 15,000 -> 15000\n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in dataset:\n",
    "    row[\"question\"] = clean(row[\"question\"])\n",
    "    row[\"passage\"] = clean(row[\"passage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'do iran and afghanistan speak the same language', 'title': 'Persian language', 'answer': True, 'passage': 'Persian (/ˈpɜːrʒən,  ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo Iranian branch of the Indo European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (947511668.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [37]\u001b[1;36m\u001b[0m\n\u001b[1;33m    print('dataset[0])[0])\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Github\\KT-Assistant\\main.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Github/KT-Assistant/main.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m sentences \u001b[39m=\u001b[39m passages\u001b[39m.\u001b[39mextend(questions)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Github/KT-Assistant/main.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m s_ \u001b[39m=\u001b[39m []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Github/KT-Assistant/main.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Github/KT-Assistant/main.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     s_\u001b[39m.\u001b[39mappend(sentence\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Github/KT-Assistant/main.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m sentences \u001b[39m=\u001b[39m s_\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "embedding_size = 100\n",
    "\n",
    "passages, questions, answers = bqd.get_split()\n",
    "\n",
    "sentences = passages.extend(questions)\n",
    "\n",
    "s_ = []\n",
    "for sentence in sentences:\n",
    "\n",
    "    s_.append(sentence.split(\" \"))\n",
    "\n",
    "sentences = s_\n",
    "\n",
    "model = Word2Vec(sentences=sentences,vector_size=embedding_size, window= 5, min_count= 1, workers= 4)\n",
    "\n",
    "model.train(sentences,total_examples=len(sentences),epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'QueryProcessor' from 'query' (C:\\Users\\laure\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\query\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Github\\KT-Assistant\\main.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Github/KT-Assistant/main.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mparse\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Github/KT-Assistant/main.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mquery\u001b[39;00m \u001b[39mimport\u001b[39;00m QueryProcessor\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Github/KT-Assistant/main.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39moperator\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Github/KT-Assistant/main.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'QueryProcessor' from 'query' (C:\\Users\\laure\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\query\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from parse import *\n",
    "from query import QueryProcessor\n",
    "import operator\n",
    "\n",
    "def main():\n",
    "\tqp = QueryParser(filename='../text/queries.txt')\n",
    "\tcp = CorpusParser(filename='../text/corpus.txt')\n",
    "\tqp.parse()\n",
    "\tqueries = qp.get_queries()\n",
    "\tcp.parse()\n",
    "\tcorpus = cp.get_corpus()\n",
    "\tproc = QueryProcessor(queries, corpus)\n",
    "\tresults = proc.run()\n",
    "\tqid = 0\n",
    "\tfor result in results:\n",
    "\t\tsorted_x = sorted(result.items(), key=operator.itemgetter(1))\n",
    "\t\tsorted_x.reverse()\n",
    "\t\tindex = 0\n",
    "\t\tfor i in sorted_x[:100]:\n",
    "\t\t\ttmp = (qid, i[0], index, i[1])\n",
    "\t\t\tprint('{:>1}\\tQ0\\t{:>4}\\t{:>2}\\t{:>12}\\tNH-BM25'.format(*tmp))\n",
    "\t\t\tindex += 1\n",
    "\t\tqid += 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
