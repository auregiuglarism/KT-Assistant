{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KT-ASSISTANT ###\n",
    "### NLP Project Group 8 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class BQDataset():\n",
    "    def __init__(self, path):\n",
    "        self.dataset = open(path,encoding=\"utf-8\")\n",
    "\n",
    "        self.dataset = [json.loads(instance) for instance in self.dataset ]\n",
    "\n",
    "        self.passages = []\n",
    "        self.questions = []\n",
    "        self.answers = []\n",
    "        self.titles = []\n",
    "\n",
    "        for inst in self.dataset:\n",
    "            self.passages.append(inst[\"passage\"])\n",
    "            self.questions.append(inst[\"question\"])\n",
    "            self.answers.append(inst[\"answer\"])\n",
    "            self.titles.append(inst[\"title\"])\n",
    "\n",
    "    def get_dataset(self):\n",
    "        return self.dataset\n",
    "\n",
    "    def get_split(self):\n",
    "        return self.passages, self.questions, self.answers\n",
    "\n",
    "bqd = BQDataset(\"datasets/train.jsonl\")\n",
    "dataset = bqd.get_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean(text, stem_words=True):\n",
    "    import re    # for regular expressions\n",
    "    from string import punctuation\n",
    "    from nltk.stem import SnowballStemmer    #if you are brave enough to do stemming\n",
    "    from nltk.corpus import stopwords      #if you want to remove stopwords\n",
    "    \n",
    "    if type(text) != str or text=='':\n",
    "        return ''\n",
    "\n",
    "    text = re.sub(\"\\'s\", \" \", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
    "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    text = re.sub(\"can't\", \"cannot\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"don't\", \"do not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"won't\", \"will not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"shouldn't\", \"should not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"couldn't\", \"could not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"isn't\", \"is not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"wasn't\", \"was not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"weren't\", \"were not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"haven't\", \"have not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"hasn't\", \"has not\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(r\"[0-9]-[0-9]\", \" minus \", text)\n",
    "    text = re.sub(\"-\", \" \", text)\n",
    "\n",
    "    digit_letters = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "    for i in range(len(digit_letters)):\n",
    "        regex = rf\"(?<=\\b){str(i)}(?=\\b)\"\n",
    "        text = re.sub(regex, digit_letters[i], text)\n",
    "    \n",
    "    # remove comma between numbers, i.e. 15,000 -> 15000\n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "\n",
    "    # Split by whitespace\n",
    "    text = text.split(\" \")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pre-processing to data\n",
    "\n",
    "for row in dataset:\n",
    "\n",
    "    row[\"question\"] = clean(row[\"question\"])\n",
    "    row[\"question\"] = tokenize(row[\"question\"])\n",
    "\n",
    "    row[\"passage\"] = clean(row[\"passage\"])\n",
    "    row[\"passage\"] = tokenize(row[\"passage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': ['do', 'iran', 'and', 'afghanistan', 'speak', 'the', 'same', 'language'], 'title': 'Persian language', 'answer': True, 'passage': ['Persian', '(/ˈpɜːrʒən,', '', 'ʃən/),', 'also', 'known', 'by', 'its', 'endonym', 'Farsi', '(فارسی', 'fārsi', '(fɒːɾˈsiː)', '(', 'listen)),', 'is', 'one', 'of', 'the', 'Western', 'Iranian', 'languages', 'within', 'the', 'Indo', 'Iranian', 'branch', 'of', 'the', 'Indo', 'European', 'language', 'family.', 'It', 'is', 'primarily', 'spoken', 'in', 'Iran,', 'Afghanistan', '(officially', 'known', 'as', 'Dari', 'since', '1958),', 'and', 'Tajikistan', '(officially', 'known', 'as', 'Tajiki', 'since', 'the', 'Soviet', 'era),', 'and', 'some', 'other', 'regions', 'which', 'historically', 'were', 'Persianate', 'societies', 'and', 'considered', 'part', 'of', 'Greater', 'Iran.', 'It', 'is', 'written', 'in', 'the', 'Persian', 'alphabet,', 'a', 'modified', 'variant', 'of', 'the', 'Arabic', 'script,', 'which', 'itself', 'evolved', 'from', 'the', 'Aramaic', 'alphabet.']}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aured\\Desktop\\Learning Material\\Bsc - DSAI UM\\Year 2\\Semester 2 Period 5\\Natural Language Processing\\NLP Project\\Code\\KT-Assistant\\main.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aured/Desktop/Learning%20Material/Bsc%20-%20DSAI%20UM/Year%202/Semester%202%20Period%205/Natural%20Language%20Processing/NLP%20Project/Code/KT-Assistant/main.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aured/Desktop/Learning%20Material/Bsc%20-%20DSAI%20UM/Year%202/Semester%202%20Period%205/Natural%20Language%20Processing/NLP%20Project/Code/KT-Assistant/main.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m embedding_size \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aured/Desktop/Learning%20Material/Bsc%20-%20DSAI%20UM/Year%202/Semester%202%20Period%205/Natural%20Language%20Processing/NLP%20Project/Code/KT-Assistant/main.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m passages, questions, answers \u001b[39m=\u001b[39m bqd\u001b[39m.\u001b[39mget_split()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "embedding_size = 100\n",
    "\n",
    "passages, questions, answers = bqd.get_split()\n",
    "\n",
    "sentences = passages.extend(questions)\n",
    "\n",
    "s_ = []\n",
    "for sentence in sentences:\n",
    "\n",
    "    s_.append(sentence.split(\" \"))\n",
    "\n",
    "sentences = s_\n",
    "\n",
    "model = Word2Vec(sentences=sentences,vector_size=embedding_size, window= 5, min_count= 1, workers= 4)\n",
    "\n",
    "model.train(sentences,total_examples=len(sentences),epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse import *\n",
    "from query import QueryProcessor\n",
    "import operator\n",
    "\n",
    "def main():\n",
    "\tqp = QueryParser(filename='../text/queries.txt')\n",
    "\tcp = CorpusParser(filename='../text/corpus.txt')\n",
    "\tqp.parse()\n",
    "\tqueries = qp.get_queries()\n",
    "\tcp.parse()\n",
    "\tcorpus = cp.get_corpus()\n",
    "\tproc = QueryProcessor(queries, corpus)\n",
    "\tresults = proc.run()\n",
    "\tqid = 0\n",
    "\tfor result in results:\n",
    "\t\tsorted_x = sorted(result.items(), key=operator.itemgetter(1))\n",
    "\t\tsorted_x.reverse()\n",
    "\t\tindex = 0\n",
    "\t\tfor i in sorted_x[:100]:\n",
    "\t\t\ttmp = (qid, i[0], index, i[1])\n",
    "\t\t\tprint('{:>1}\\tQ0\\t{:>4}\\t{:>2}\\t{:>12}\\tNH-BM25'.format(*tmp))\n",
    "\t\t\tindex += 1\n",
    "\t\tqid += 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
