{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KT-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class BQDataset():\n",
    "    def __init__(self, path):\n",
    "        self.dataset = open(path,encoding=\"utf-8\")\n",
    "\n",
    "        self.dataset = [json.loads(instance) for instance in self.dataset ]\n",
    "\n",
    "\n",
    "        self.passages = []\n",
    "        self.questions = []\n",
    "        self.answers = []\n",
    "        self.titles = []\n",
    "\n",
    "        for inst in self.dataset:\n",
    "            self.passages.append(inst[\"passage\"])\n",
    "            self.questions.append(inst[\"question\"])\n",
    "            self.answers.append(inst[\"answer\"])\n",
    "            self.titles.append(inst[\"title\"])\n",
    "\n",
    "    def get_dataset(self):\n",
    "        return self.dataset\n",
    "\n",
    "    def get_split(self):\n",
    "\n",
    "        return self.passages, self.questions, self.answers\n",
    "#\n",
    "\n",
    "\n",
    "bqd = BQDataset(\"datasets/train.jsonl\")\n",
    "dataset = bqd.get_dataset()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean(text, stem_words=True):\n",
    "    import re    # for regular expressions\n",
    "    from string import punctuation\n",
    "    from nltk.stem import SnowballStemmer    #if you are brave enough to do stemming\n",
    "    from nltk.corpus import stopwords      #if you want to remove stopwords\n",
    "    \n",
    "    # Empty question\n",
    "    \n",
    "    if type(text) != str or text=='':\n",
    "        return ''\n",
    "\n",
    "    # Clean the text (here i have 2-3 cases of pre-processing by sampling the data. You might need more)\n",
    "    text = re.sub(\"\\'s\", \" \", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
    "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    text = re.sub(\"can't\", \"cannot\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"don't\", \"do not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"won't\", \"will not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"shouldn't\", \"should not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"couldn't\", \"could not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"isn't\", \"is not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"wasn't\", \"was not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"weren't\", \"were not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"haven't\", \"have not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"hasn't\", \"has not\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(r\"[0-9]-[0-9]\", \" minus \", text)\n",
    "    text = re.sub(\"-\", \" \", text)\n",
    "\n",
    "\n",
    "\n",
    "    digit_letters = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "    for i in range(len(digit_letters)):\n",
    "        regex = rf\"(?<=\\b){str(i)}(?=\\b)\"\n",
    "        text = re.sub(regex, digit_letters[i], text)\n",
    "\n",
    "\n",
    "    #you might need more regex\n",
    "    #add them here\n",
    " \n",
    "    ### YOUR CODE ENDS HERE\n",
    "    \n",
    "    # remove comma between numbers, i.e. 15,000 -> 15000\n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in dataset:\n",
    "    row[\"question\"] = clean(row[\"question\"])\n",
    "    row[\"passage\"] = clean(row[\"passage\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'do iran and afghanistan speak the same language', 'title': 'Persian language', 'answer': True, 'passage': 'Persian (/ˈpɜːrʒən,  ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo Iranian branch of the Indo European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check our results. Try different samples and see if you are satisfied with the performance.\n",
    "\n",
    "a = 401 \n",
    "for i in range(a,a+10):\n",
    "    print(df_train.question1[i])\n",
    "    print(df_train.question2[i])\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "embedding_size = 100\n",
    "\n",
    "passages, questions, answers = bqd.get_split()\n",
    "\n",
    "sentences = passages.extend(questions)\n",
    "\n",
    "s_ = []\n",
    "for sentence in sentences:\n",
    "\n",
    "    s_.append(sentence.split(\" \"))\n",
    "\n",
    "sentences = s_\n",
    "\n",
    "model = Word2Vec(sentences=sentences,vector_size=embedding_size, window= 5, min_count= 1, workers= 4)\n",
    "\n",
    "model.train(sentences,total_examples=len(sentences),epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse import *\n",
    "from query import QueryProcessor\n",
    "import operator\n",
    "\n",
    "def main():\n",
    "\tqp = QueryParser(filename='../text/queries.txt')\n",
    "\tcp = CorpusParser(filename='../text/corpus.txt')\n",
    "\tqp.parse()\n",
    "\tqueries = qp.get_queries()\n",
    "\tcp.parse()\n",
    "\tcorpus = cp.get_corpus()\n",
    "\tproc = QueryProcessor(queries, corpus)\n",
    "\tresults = proc.run()\n",
    "\tqid = 0\n",
    "\tfor result in results:\n",
    "\t\tsorted_x = sorted(result.items(), key=operator.itemgetter(1))\n",
    "\t\tsorted_x.reverse()\n",
    "\t\tindex = 0\n",
    "\t\tfor i in sorted_x[:100]:\n",
    "\t\t\ttmp = (qid, i[0], index, i[1])\n",
    "\t\t\tprint('{:>1}\\tQ0\\t{:>4}\\t{:>2}\\t{:>12}\\tNH-BM25'.format(*tmp))\n",
    "\t\t\tindex += 1\n",
    "\t\tqid += 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
